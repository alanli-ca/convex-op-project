{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack\n",
    "import pdb\n",
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "def print_results(predictions, labels, name):\n",
    "    print(\"{}: {}\".format(name, (predictions == labels).sum() / float(len(labels))))\n",
    "\n",
    "def pad_zeros(input_vector, output_size):\n",
    "    pad = np.zeros(output_size - input_vector.shape[0])\n",
    "    return np.append(input_vector, pad)\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def graph_to_degree_hist(G):\n",
    "    degree_hist = np.array(nx.degree_histogram(G))\n",
    "    degree_hist_padded = pad_zeros(degree_hist, nx.number_of_nodes(G))\n",
    "    return degree_hist_padded\n",
    "\n",
    "def plot_degree_histograms(data, labels, prefix=\"\"):\n",
    "    classes = sorted(list(set(labels)))\n",
    "    num_classes = len(classes)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    f, axarr = plt.subplots(num_classes, sharex=True, sharey=True)\n",
    "    f.suptitle(\"{}: Average Node Degree Histogram\".format(prefix))\n",
    "    \n",
    "    for counter, class_category in enumerate(classes):\n",
    "        idx = np.where(labels == class_category)\n",
    "        X = data[idx]\n",
    "        mean_hist = np.mean(np.array(X), axis=0)\n",
    "        axarr[counter].bar(np.arange(data.shape[1]), mean_hist, label=\"p: \"+class_category)\n",
    "        \n",
    "    f.subplots_adjust(hspace=0)\n",
    "    for ax in axarr:\n",
    "        ax.label_outer()\n",
    "        ax.legend()\n",
    "        \n",
    "def visualization(data, labels, method=\"tSNE\"):\n",
    "    if method == \"tSNE\":\n",
    "        viz = TSNE(n_components=2, random_state=0)\n",
    "    elif method == \"PCA\":\n",
    "        viz = PCA(n_components=2)\n",
    "    data_2d = viz.fit_transform(data)\n",
    "    label_classes = np.unique(labels).tolist()\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple']\n",
    "    for i, c in zip(label_classes, colors):\n",
    "        plt.scatter(data_2d[labels == i, 0], data_2d[labels == i, 1], c=c, label=i)\n",
    "    plt.title(\"{} visualization of graph classes\".format(method))\n",
    "    plt.legend()\n",
    "    \n",
    "def load_data(train_path, test_path):\n",
    "    train_list = [f for f in os.listdir(train_path) if not f.startswith('.')]\n",
    "    test_list = [f for f in os.listdir(test_path) if not f.startswith('.')]\n",
    "    train_list.sort()\n",
    "    test_list.sort()\n",
    "\n",
    "    graphs_train = []\n",
    "    graphs_test = []\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "\n",
    "    for filename in train_list:\n",
    "        graphs_train.append(nx.read_adjlist(train_path+filename))\n",
    "        labels_train.append(filename.split(\"_\")[2][:3])\n",
    "\n",
    "    for filename in test_list:\n",
    "        graphs_test.append(nx.read_adjlist(test_path+filename))\n",
    "        labels_test.append(filename.split(\"_\")[2][:3])\n",
    "\n",
    "    return graphs_train, graphs_test, labels_train, labels_test\n",
    "\n",
    "def WL_compute(ad_list, node_label, h):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    - ad_list: the graphs are stored in lists; each element is a list itself containing the adjancency lists\n",
    "    - node_label: list of lists, where each entry is a list of node labels for the graph\n",
    "    - h: iterations of WL\n",
    "\n",
    "    OUTPUTS\n",
    "    - K: List of kernel matrices at each iteration\n",
    "    - phi_list: list of features matrices\n",
    "\n",
    "    \"\"\"\n",
    "    # Total number of graphs in the dataset\n",
    "    n = len(ad_list)\n",
    "    \n",
    "    # Total number of nodes in dataset: initialized as zero\n",
    "    tot_nodes = 0\n",
    "\n",
    "\n",
    "    # list of kernel matrices\n",
    "    K = [0]*(h+1)\n",
    "    # list of feature mtrices\n",
    "    phi_list = [0] * (h+1)\n",
    "\n",
    "    #total number of nodes in the dataset\n",
    "    for i in range(n):\n",
    "        tot_nodes = tot_nodes + int(len(ad_list[i]))\n",
    "\n",
    "    \n",
    "    #each column of phi will be the explicit feature representation for the graph j\n",
    "    phi = lil_matrix((tot_nodes, n), dtype = np.uint32)\n",
    "\n",
    "    # labels will be used to store the new labels\n",
    "    labels = [0] * n\n",
    "\n",
    "    #label lookup is a dictionary which will contain the mapping\n",
    "    # from multiset labels (strings) to short labels (integers)\n",
    "    label_lookup = {}\n",
    "\n",
    "    # counter to create possibly new labels in the update step\n",
    "    label_counter = 0\n",
    "\n",
    "    # Note: here we are just renaming the node labels from 0,..,num_labels\n",
    "    # for each graph\n",
    "    for i in range(n):\n",
    "\n",
    "        # copy the original labels\n",
    "        l_aux = np.copy(node_label[i])\n",
    "\n",
    "        # will be used to store the new labels\n",
    "        labels[i] = np.zeros(len(l_aux), dtype = np.int32)\n",
    "\n",
    "        # for each label in graph\n",
    "        for j in range(len(l_aux)):\n",
    "            l_aux_str = str(l_aux[j])\n",
    "\n",
    "            # If the string do not already exist\n",
    "            # then create a new short label\n",
    "            if l_aux_str not in label_lookup:\n",
    "                label_lookup[l_aux_str] = label_counter\n",
    "                labels[i][j] = label_counter                                                              \n",
    "                label_counter += 1\n",
    "            else:\n",
    "                labels[i][j] = label_lookup[l_aux_str]\n",
    "\n",
    "            # node histograph of the new labels\n",
    "            phi[labels[i][j],i] += 1\n",
    "\n",
    "    L = label_counter\n",
    "    print('Number of original labels %d' %L)\n",
    "\n",
    "    #####################\n",
    "    # --- Main code --- #\n",
    "    #####################\n",
    "\n",
    "    # Now we are starting with the first iteration of WL\n",
    "\n",
    "    # features obtained from the original node (renamed) labels\n",
    "    phi_list[0] = phi\n",
    "\n",
    "    # Kernel matrix based on original features\n",
    "    K[0] = phi.transpose().dot(phi).toarray().astype(np.float32)\n",
    "    \n",
    "    print(\"K original is computed\")\n",
    "    \n",
    "    # Initialize iterations to 0\n",
    "    it = 0\n",
    "\n",
    "    # copy of the original labels: will stored the new labels\n",
    "    new_labels = np.copy(labels)\n",
    "    \n",
    "    # until the number of iterations is less than h\n",
    "    while it < h:\n",
    "\n",
    "        # Initialize dictionary and counter \n",
    "        # (same meaning as before)        \n",
    "        label_lookup = {}\n",
    "        label_counter = 0\n",
    "\n",
    "        # Initialize phi as a sparse matrix\n",
    "        phi = lil_matrix((tot_nodes, n), dtype = np.int32)\n",
    "        # convert it to array\n",
    "        phi = phi.toarray()\n",
    "\n",
    "        print(\"Iteration %d: phi is computed\" % it)\n",
    "\n",
    "        # for each graph in the dataset\n",
    "        for i in range(n):\n",
    "\n",
    "            # will store the multilabel string\n",
    "            l_aux_long = np.copy(labels[i])\n",
    "\n",
    "            # for each node in graph\n",
    "            for v in range(len(ad_list[i])):\n",
    "\n",
    "                # the new labels convert to tuple\n",
    "                new_node_label = tuple([l_aux_long[v]]) \n",
    "\n",
    "                # form a multiset label of the node neighbors \n",
    "                new_ad = np.zeros(len(ad_list[i][v]))\n",
    "                for j in range(len(ad_list[i][v])):\n",
    "                    new_ad[j] = ad_list[i][v][j]\n",
    "                new_ad = new_ad.astype(int)\n",
    "                ad_aux = tuple([l_aux_long[j] for j in new_ad])\n",
    "\n",
    "                # long labels: original node plus sorted neughbors\n",
    "                long_label = tuple(tuple(new_node_label)+tuple(sorted(ad_aux)))\n",
    "            \n",
    "                # if the multiset label has not yet occurred , add\n",
    "                # it to the lookup table and assign a number to it\n",
    "                if long_label not in label_lookup:\n",
    "                    label_lookup[long_label] = str(label_counter)\n",
    "                    new_labels[i][v] = str(label_counter)\n",
    "                    label_counter += 1\n",
    "\n",
    "                # else assign it the already existing number\n",
    "                else:\n",
    "                    new_labels[i][v] = label_lookup[long_label]\n",
    "\n",
    "            # count the node label frequencies\n",
    "            aux = np.bincount(new_labels[i]) \n",
    "            phi[new_labels[i],i] += aux[new_labels[i]]\n",
    "        \n",
    "        L = label_counter\n",
    "        print('Number of compressed labels %d' %L)\n",
    "\n",
    "        # create phi for iteration it+1\n",
    "        phi_sparse = lil_matrix(phi)\n",
    "        phi_list[it+1] = phi_sparse\n",
    "\n",
    "        print(\"Itaration %d: phi sparse saved\" % it)\n",
    "\n",
    "        # create K at iteration it+1\n",
    "        K[it+1] = K[it] + phi_sparse.transpose().dot(phi_sparse).toarray().astype(np.float32)\n",
    "       \n",
    "        print(\"Iteration %d: K is computed\" % it)\n",
    "\n",
    "        # Initialize labels for the next iteration as the new just computed\n",
    "        labels = copy.deepcopy(new_labels)\n",
    "\n",
    "        # increment the iteration\n",
    "        it = it + 1 \n",
    "\n",
    "    return K, phi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset_path = \"test_1/\"\n",
    "train_path = \"./data/\"+dataset_path+\"train/\"\n",
    "test_path = \"./data/\"+dataset_path+\"test/\"\n",
    "graphs_train, graphs_test, labels_train, labels_test = load_data(train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original labels 12\n",
      "K original is computed\n",
      "Iteration 0: phi is computed\n",
      "Number of compressed labels 3761\n",
      "Itaration 0: phi sparse saved\n",
      "Iteration 0: K is computed\n",
      "Iteration 1: phi is computed\n",
      "Number of compressed labels 19890\n",
      "Itaration 1: phi sparse saved\n",
      "Iteration 1: K is computed\n",
      "Iteration 2: phi is computed\n",
      "Number of compressed labels 20000\n",
      "Itaration 2: phi sparse saved\n",
      "Iteration 2: K is computed\n",
      "Iteration 3: phi is computed\n",
      "Number of compressed labels 20000\n",
      "Itaration 3: phi sparse saved\n",
      "Iteration 3: K is computed\n",
      "Iteration 4: phi is computed\n",
      "Number of compressed labels 20000\n",
      "Itaration 4: phi sparse saved\n",
      "Iteration 4: K is computed\n",
      "(400, 400)\n",
      "(200, 200)\n",
      "(200, 200)\n"
     ]
    }
   ],
   "source": [
    "ad_list_train = []\n",
    "node_label_train = []\n",
    "ad_list_test = []\n",
    "node_label_test = []\n",
    "\n",
    "for graph in graphs_train:\n",
    "    al = graph.adjacency_list()\n",
    "    nl = nx.degree(graph).values()\n",
    "    nl = [str(i) for i in nl]\n",
    "    ad_list_train.append(al)\n",
    "    node_label_train.append(nl)\n",
    "\n",
    "for graph in graphs_test:\n",
    "    al = graph.adjacency_list()\n",
    "    nl = nx.degree(graph).values()\n",
    "    nl = [str(i) for i in nl]\n",
    "    ad_list_test.append(al)\n",
    "    node_label_test.append(nl)\n",
    "    \n",
    "ad_list = ad_list_train + ad_list_test\n",
    "node_label_list = node_label_train + node_label_test\n",
    "    \n",
    "kernel_list, phi_list = WL_compute(ad_list, node_label_list, 5)\n",
    "\n",
    "kernel_mat = np.asarray(kernel_list[-1])\n",
    "train_kernel_mat = kernel_mat[:len(graphs_train), :len(graphs_train)]\n",
    "test_kernel_mat = kernel_mat[len(graphs_train):, :len(graphs_train)]\n",
    "\n",
    "print(np.asarray(kernel_mat).shape)\n",
    "print(np.asarray(train_kernel_mat).shape)\n",
    "print(np.asarray(test_kernel_mat).shape)\n",
    "\n",
    "train_labels = np.array(labels_train)\n",
    "test_labels = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "classifiers = {\n",
    "    \"svm\": SVC()\n",
    "}\n",
    "\n",
    "parameters_search = {\n",
    "    \"svm\": {'kernel':['precomputed'], 'C':[1]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "best score: 1.0 | best params: {'C': 1, 'kernel': 'precomputed'}\n"
     ]
    }
   ],
   "source": [
    "# select model\n",
    "model = classifiers[\"svm\"]\n",
    "parameters = parameters_search[\"svm\"]\n",
    "clf = GridSearchCV(model, parameters)\n",
    "\n",
    "# Training session 1\n",
    "clf.fit(train_kernel_mat, train_labels)\n",
    "train_predictions = clf.predict(train_kernel_mat).reshape(-1,1)\n",
    "test_predictions = clf.predict(test_kernel_mat).reshape(-1,1)\n",
    "print_results(np.ravel(train_predictions), train_labels, \"Train Accuracy\")\n",
    "print_results(np.ravel(test_predictions), test_labels, \"Test Accuracy\")\n",
    "print(\"best score: {} | best params: {}\".format(clf.best_score_, clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
